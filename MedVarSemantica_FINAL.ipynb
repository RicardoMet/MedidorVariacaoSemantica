{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY87B16V73D5"
      },
      "source": [
        "# Análise de Variabilidade Semântica (Colab)\n",
        "\n",
        "Notebook para executar o script `analise_variabilidade.py` passo a passo no Google Colab.\n",
        "\n",
        "Escolhe a célula de instalação, depois a de configuração (path para o ficheiro no Drive) e corre todas."
      ],
      "id": "lY87B16V73D5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxFxJPUF73D8"
      },
      "source": [
        "# Instalar dependências (executar apenas se necessário)\n",
        "!pip install -q spacy openpyxl beautifulsoup4 lxml nltk\n",
        "!python -m spacy download pt_core_news_lg\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "mxFxJPUF73D8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kqApf3O73D9"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import wordnet as wn\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "print('Configuração carregada.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6kqApf3O73D9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsXT0axY73D9"
      },
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "dsXT0axY73D9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7df2qpe73D-"
      },
      "source": [
        "# CONFIGURAÇÃO: caminho do ficheiro XML no teu Drive e tipo de construção\n",
        "FILE_PATH = '/content/drive/MyDrive/Constructions_concordances/SVO_7000.xml'  # ALTERA AQUI\n",
        "tipo_construcao = 'svo'  # 'svo', 'n_adj', 'adj_n'\n",
        "print('FILE_PATH =', FILE_PATH)\n",
        "print('tipo_construcao =', tipo_construcao)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a7df2qpe73D-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxG7SXSH73D-"
      },
      "source": [
        "def limpar_kwic(texto_kwic):\n",
        "    return re.sub(r\"/[a-zA-Z]+\", \"\", texto_kwic)\n",
        "\n",
        "def extrair_svo(frase):\n",
        "    doc = nlp(frase)\n",
        "    sujeito = verbo = objeto = None\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
        "            verbo = token.lemma_\n",
        "        elif token.dep_ == 'nsubj':\n",
        "            sujeito = token.text\n",
        "        elif token.dep_ in {'obj','dobj','obl','attr'}:\n",
        "            objeto = token.lemma_\n",
        "    if verbo and (sujeito or objeto):\n",
        "        return {'frase_limpa': frase, 'sujeito': sujeito or '', 'verbo': verbo, 'objeto': objeto or ''}\n",
        "    return None\n",
        "\n",
        "def extrair_n_adj(frase):\n",
        "    doc = nlp(frase)\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'ADJ' and token.head.pos_ == 'NOUN':\n",
        "            return {'frase_limpa': frase, 'nome': token.head.lemma_, 'adjetivo': token.lemma_}\n",
        "    return None\n",
        "\n",
        "def extrair_adj_n(frase):\n",
        "    doc = nlp(frase)\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'ADJ' and token.head.pos_ == 'NOUN' and token.i < token.head.i:\n",
        "            return {'frase_limpa': frase, 'adjetivo': token.lemma_, 'nome': token.head.lemma_}\n",
        "    for i in range(len(doc)-1):\n",
        "        if doc[i].pos_ == 'ADJ' and doc[i+1].pos_ == 'NOUN':\n",
        "            return {'frase_limpa': frase, 'adjetivo': doc[i].lemma_, 'nome': doc[i+1].lemma_}\n",
        "    return None\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "sxG7SXSH73D-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQQLIgZ173D-"
      },
      "source": [
        "# Leitura do XML\n",
        "if not os.path.exists(FILE_PATH):\n",
        "    raise FileNotFoundError(f'Arquivo não encontrado: {FILE_PATH}')\n",
        "\n",
        "with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "    xml = f.read()\n",
        "\n",
        "soup = BeautifulSoup(xml, 'xml')\n",
        "kwics = soup.find_all('kwic')\n",
        "kwic_pairs = [(limpar_kwic(k.text.strip()), k.text.strip()) for k in kwics]\n",
        "\n",
        "print(f'KWICs lidos: {len(kwic_pairs)}')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "lQQLIgZ173D-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ropVkG173D_"
      },
      "source": [
        "# Construção do DataFrame\n",
        "dados = []\n",
        "for frase_limpa, frase_original in kwic_pairs:\n",
        "    if tipo_construcao == 'svo':\n",
        "        extraido = extrair_svo(frase_limpa)\n",
        "    elif tipo_construcao == 'n_adj':\n",
        "        extraido = extrair_n_adj(frase_limpa)\n",
        "    elif tipo_construcao == 'adj_n':\n",
        "        extraido = extrair_adj_n(frase_limpa)\n",
        "    else:\n",
        "        extraido = None\n",
        "    if extraido:\n",
        "        extraido['frase_original'] = frase_original\n",
        "        extraido['frase_limpa'] = frase_limpa\n",
        "        dados.append(extraido)\n",
        "\n",
        "df = pd.DataFrame(dados)\n",
        "print(f'Frases extraídas: {len(df)}')\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3ropVkG173D_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJtJ1o3d73D_"
      },
      "source": [
        "# Mapeamento domínios (igual ao script)\n",
        "mapeamento_dominios = {\n",
        "    'noun.person': 'pessoa',\n",
        "    'noun.artifact': 'objeto',\n",
        "    'noun.act': 'evento',\n",
        "    'noun.event': 'evento',\n",
        "    'noun.group': 'organização',\n",
        "    'noun.location': 'lugar',\n",
        "    'noun.communication': 'comunicação',\n",
        "    'noun.state': 'estado',\n",
        "    'noun.cognition': 'conhecimento',\n",
        "    'noun.quantity': 'quantidade',\n",
        "    'noun.attribute': 'característica',\n",
        "    'noun.time': 'tempo',\n",
        "    'noun.animal': 'animal',\n",
        "    'noun.body': 'corpo',\n",
        "    'noun.food': 'comida',\n",
        "    'noun.substance': 'matéria',\n",
        "    'noun.object': 'objeto',\n",
        "    'noun.feeling': 'emoção',\n",
        "    'noun.phenomenon': 'fenómeno',\n",
        "}\n",
        "\n",
        "def obter_dominios_note(book_word, lang='por'):\n",
        "    if not book_word:\n",
        "        return 'desconhecido', 'desconhecido'\n",
        "    synsets = wn.synsets(book_word, lang=lang)\n",
        "    if not synsets:\n",
        "        return 'desconhecido', 'desconhecido'\n",
        "    primeiro = synsets[0]\n",
        "    subdominio = primeiro.lexname()\n",
        "    dominio_mapeado = mapeamento_dominios.get(subdominio, 'outro')\n",
        "    return dominio_mapeado, subdominio\n",
        "\n",
        "# Atribui domínios no DataFrame (conforme tipo de construção)\n",
        "if tipo_construcao == 'svo':\n",
        "    df['dominio'], df['subdominio'] = zip(*df['objeto'].apply(lambda x: obter_dominios_note(x)))\n",
        "    df['dominio_sujeito'], df['subdominio_sujeito'] = zip(*df['sujeito'].apply(lambda x: obter_dominios_note(x)))\n",
        "    df['construcao'] = df['verbo'].apply(lambda v: f\"{v} X\")\n",
        "elif tipo_construcao == 'n_adj':\n",
        "    df['dominio'], df['subdominio'] = zip(*df['nome'].apply(lambda x: obter_dominios_note(x)))\n",
        "    df['construcao'] = df.apply(lambda r: f\"{r['nome']} + {r['adjetivo']}\", axis=1)\n",
        "elif tipo_construcao == 'adj_n':\n",
        "    df['dominio'], df['subdominio'] = zip(*df['nome'].apply(lambda x: obter_dominios_note(x)))\n",
        "    df['construcao'] = df.apply(lambda r: f\"{r['adjetivo']} {r['nome']}\", axis=1)\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "YJtJ1o3d73D_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPqmpagu73EA"
      },
      "source": [
        "# Cálculo de variabilidade (SVO com verbo->obj e verbo->suj)\n",
        "if tipo_construcao == 'svo':\n",
        "    agrupados_verbo_obj = df.groupby('verbo')['dominio'].apply(list)\n",
        "    df_var_verbo_obj = agrupados_verbo_obj.apply(lambda x: len(set(x))).reset_index(name='variabilidade_verbo_obj')\n",
        "\n",
        "    agrupados_verbo_suj = df.groupby('verbo')['dominio_sujeito'].apply(list)\n",
        "    df_var_verbo_suj = agrupados_verbo_suj.apply(lambda x: len(set(x))).reset_index(name='variabilidade_verbo_suj')\n",
        "\n",
        "    display(df_var_verbo_obj.sort_values(by='variabilidade_verbo_obj', ascending=False).head(10))\n",
        "    display(df_var_verbo_suj.sort_values(by='variabilidade_verbo_suj', ascending=False).head(10))\n",
        "elif tipo_construcao == 'n_adj':\n",
        "    df['dominio_adjetivo'], df['subdominio_adjetivo'] = zip(*df['adjetivo'].apply(lambda x: obter_dominios_note(x)))\n",
        "    agrupados_nome = df.groupby('nome')['dominio_adjetivo'].apply(list)\n",
        "    df_var_nome = agrupados_nome.apply(lambda x: len(set(x))).reset_index(name='variabilidade_nome')\n",
        "    display(df_var_nome.sort_values(by='variabilidade_nome', ascending=False).head(10))\n",
        "elif tipo_construcao == 'adj_n':\n",
        "    agrupados = df.groupby('adjetivo')['dominio'].apply(list)\n",
        "    df_var = agrupados.apply(lambda x: len(set(x))).reset_index(name='variabilidade_semantica')\n",
        "    display(df_var.sort_values(by='variabilidade_semantica', ascending=False).head(10))\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "gPqmpagu73EA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2oeoKU673EA"
      },
      "source": [
        "# Exportar para Excel no Drive\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_path = f\"/content/drive/MyDrive/Constructions_concordances/output_{tipo_construcao}_{timestamp}.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "    df.to_excel(writer, index=False, sheet_name='Construcoes')\n",
        "    if tipo_construcao == 'svo':\n",
        "        df_var_verbo_obj.to_excel(writer, index=False, sheet_name='Variabilidade_verbo_objeto')\n",
        "        df_var_verbo_suj.to_excel(writer, index=False, sheet_name='Variabilidade_verbo_sujeito')\n",
        "    elif tipo_construcao == 'n_adj':\n",
        "        df_var_nome.to_excel(writer, index=False, sheet_name='Variabilidade_nome')\n",
        "    elif tipo_construcao == 'adj_n':\n",
        "        df_var.to_excel(writer, index=False, sheet_name='Variabilidade')\n",
        "\n",
        "print('✅ Exportado para:', output_path)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Y2oeoKU673EA"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}